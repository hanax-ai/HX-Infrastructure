# LiteLLM proxy configuration - OpenAI-compatible
# Generated by Ansible role hx_litellm_proxy
# Documentation: https://docs.litellm.ai/

model_list:
{% for m in litellm_models %}
{%   for b in litellm_backends %}
  - model_name: {{ m.name | quote }}
    litellm_params:
      model: {{ m.provider | default('ollama') }}/{{ m.model | quote }}
      api_base: {{ b | quote }}
      api_key: ""  # Ollama doesn't require API key
      request_timeout: 30
      mode: "chat"
    max_input_tokens: 2048
    num_retries: 2
{%   endfor %}
{% endfor %}

router_settings:
  routing_strategy: "least-busy"
  request_timeout: 30
  
  # Health check settings
  health_check_interval: 60
  health_check_timeout: 30

general_settings:
  # Authentication
  master_key: {{ litellm_master_key | quote }}
  
  # Database (for virtual key management)
{% if litellm_database_url is defined %}
  database_url: {{ litellm_database_url | quote }}
{% else %}
  database_url: "postgresql://{{ litellm_db_user }}:{{ litellm_db_password }}@{{ litellm_db_host }}:{{ litellm_db_port | default(5432) }}/{{ litellm_db_name }}"
{% endif %}
  
  # Telemetry
  telemetry: false
  
  # Cache settings
  cache: false
  # cache_params:
  #   type: "redis"
  #   host: "localhost"
  #   port: 6379
  
  # Logging
  json_logs: false
  log_level: "INFO"
  
  # CORS settings
  cors_allow_origins: ["*"]
  cors_allow_credentials: true
  cors_allow_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"]
  cors_allow_headers: ["*"]

# litellm_settings are passed to litellm.completion() calls
litellm_settings:
  drop_params: true
  set_verbose: false
  
  # Model fallbacks
  fallbacks: []
  
  # Context window fallbacks
  context_window_fallbacks: []
  
  # Success callback
  success_callback: []
  
  # Failure callback
  failure_callback: []

# Optional: Environment-specific settings
environment_variables:
  # OPENAI_API_KEY: ""  # Not needed for Ollama
  # ANTHROPIC_API_KEY: ""  # Example for future providers